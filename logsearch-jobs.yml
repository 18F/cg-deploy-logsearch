jobs:
###############################################################################
#First deploy group - elasticsearch_master, cluster_monitor, queue, maintenance
###############################################################################
- name: elasticsearch_master
  instances: 1
  templates:
  - {name: elasticsearch, release: logsearch}
  - {name: riemann-emitter, release: riemann}
  - {name: riemann-checklogs, release: riemann}
  resource_pool: elasticsearch_master
  persistent_disk_pool: elasticsearch_master
  networks:
  - name: default
    static_ips: (( static_ips(0) ))
  properties:
    elasticsearch:
      node:
        allow_master: true
        allow_data: false
  update:
    max_in_flight: 1 # Should never update more than one ES master node at a time or cluster will go down

- name: cluster_monitor
  instances: 1
  templates:
  - {release: logsearch, name: queue}
  - {release: logsearch, name: parser}
  - {release: logsearch, name: ingestor_syslog}
  - {release: logsearch, name: elasticsearch}
  - {release: logsearch, name: elasticsearch_config}
  - {release: logsearch, name: curator}
  - {release: logsearch, name: kibana}
  - {release: logsearch, name: nats_to_syslog}
  - {name: apply-patch, release: logstash-output-s3-backport}
  resource_pool: cluster_monitor
  networks:
  - name: default
    static_ips: (( static_ips(2) ))
  persistent_disk_pool: cluster_monitor
  properties:
    kibana:
      port: 5601
      elasticsearch:
        host: 127.0.0.1
        port: 9200
      memory_limit: 30
      wait_for_templates: [shards-and-replicas]
    elasticsearch:
      master_hosts: [127.0.0.1]
      cluster_name: monitor
      node:
        allow_master: true
        allow_data: true
    redis:
      host: 127.0.0.1
      maxmemory: 10
    curator:
      elasticsearch_host: 127.0.0.1
      elasticsearch_port: 9200
      purge_logs:
        retention_period: 7
    elasticsearch_config:
      elasticsearch:
        host: 127.0.0.1
        port: 9200
      templates:
      - shards-and-replicas: "{ \"template\" : \"*\", \"order\" : 99, \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } }"
      - index_template: /var/vcap/packages/logsearch-config/default-mappings.json
    logstash_parser:
      filters:
      - monitor: /var/vcap/packages/logsearch-config/logstash-filters-monitor.conf
    nats_to_syslog:
      nats:
        subject: ">"
        user: nats
        password: nats-password
        port: 4222
        machines: []
      debug: true
      syslog:
        host: 127.0.0.1
        port: 514
    logstash_ingestor:
      syslog:
        port: 514

- name: queue
  instances: 1
  templates:
  - {name: queue, release: logsearch}
  resource_pool: queue
  persistent_disk_pool: queue
  networks:
  - name: default
    static_ips: (( static_ips(3) ))

- name: maintenance
  instances: 1
  templates:
  - {name: curator, release: logsearch}
  - {name: elasticsearch_config, release: logsearch}
  - {name: logsearch-for-cloudfoundry-filters, release: logsearch-for-cloudfoundry}
  resource_pool: maintenance
  networks:
  - name: default
  properties:
    elasticsearch_config:
      templates:
      - index_template: /var/vcap/packages/logsearch-for-cloudfoundry-filters/logs-template.json
  update:
    serial: true # Block on this job to create deploy group 1

##################################################################
#2nd deploy group - elasticsearch_data, kibana, ingestors, parsers
##################################################################

- name: elasticsearch_data
  instances: 2
  templates:
  - {name: elasticsearch, release: logsearch}
  resource_pool: elasticsearch_data
  persistent_disk_pool: elasticsearch_data
  networks:
  - name: default
    static_ips: (( static_ips(16, 17) ))
  properties:
    elasticsearch:
      node:
        allow_master: false
        allow_data: true
  update:
    max_in_flight: 1 # Only update 1 ES data node at a time or risk downtime

- name: kibana
  instances: 1
  templates:
  - {name: kibana, release: logsearch}
  - {name: kibana-auth-plugin, release: logsearch-for-cloudfoundry}
  resource_pool: kibana
  networks:
  - name: default
    static_ips: (( static_ips(5) ))
  properties:
    kibana:
      default_app_id: "dashboard/App-Overview"
      plugins:
      - auth: /var/vcap/packages/kibana-auth-plugin/kibana-auth-plugin.tar.gz

- name: ingestor
  instances: 1
  templates:
  - {name: ingestor_relp, release: logsearch}
  - {name: ingestor_syslog, release: logsearch}
  - {name: ingestor_cloudfoundry-firehose, release: logsearch-for-cloudfoundry}
  - {name: apply-patch, release: logstash-output-s3-backport}
  resource_pool: ingestor
  networks:
  - name: default
    static_ips: (( static_ips(1) ))
  properties:
    cloudfoundry:
      firehose_events: "LogMessage,ContainerMetric,Error"
    logstash_ingestor:
      debug: false
      relp:
        port: ~
    syslog:
      host: (( grab jobs.ingestor.networks.default.static_ips.[0] ))
      port: 5514

- name: parser
  instances: 2
  templates:
  - {name: parser, release: logsearch}
  - {name: elasticsearch, release: logsearch}
  - {name: logsearch-for-cloudfoundry-filters, release: logsearch-for-cloudfoundry}
  resource_pool: parser
  networks:
  - name: default
  properties:
    logstash_parser:
      elasticsearch_index: "logs-%{[@metadata][index]}-%{+YYYY.MM.dd}"
      elasticsearch_index_type: "%{[@metadata][type]}"
      filters:
      - logsearch-for-cf: /var/vcap/packages/logsearch-for-cloudfoundry-filters/logstash-filters-default.conf
  update:
    serial: false # Block to create deploy group 2
    max_in_flight: 4  # Its ok to update multiple parsers at a time

####################################################
#3nd deploy group - ls-router (haproxy), and errands
####################################################

- name: ls-router
  instances: 1
  templates:
  - { name: haproxy, release: logsearch }
  - { name: route_registrar, release: cf }
  resource_pool: haproxy
  networks:
  - name: default
    default: [gateway, dns]
    static_ips: (( static_ips(9) ))
  properties:
    haproxy:
      syslog_server: (( grab jobs.cluster_monitor.networks.[0].static_ips.[0] ))
      ingestor:
        backend_servers: (( grab jobs.ingestor.networks.[0].static_ips ))
      kibana:
        backend_servers: (( grab jobs.kibana.networks.[0].static_ips ))
      cluster_monitor:
        backend_servers: (( grab jobs.cluster_monitor.networks.[0].static_ips ))

- name: enable_shard_allocation
  instances: 1
  lifecycle: errand
  templates:
  - {name: enable_shard_allocation, release: logsearch}
  resource_pool: errand
  networks:
  - name: default
  properties:
    enable_shard_allocation:
      elasticsearch:
        master_node: (( grab jobs.elasticsearch_master.networks.default.static_ips.[0] ))

- name: smoke-tests
  instances: 1
  resource_pool: errand
  release: logsearch
  templates:
  - {name: smoke-tests, release: logsearch}
  networks:
  - name: default
  lifecycle: errand
  properties:
    smoke_tests:
      syslog_ingestor:
        host: (( grab jobs.ls-router.networks.[0].static_ips.[0] ))
      elasticsearch_master:
        host: (( grab jobs.elasticsearch_master.networks.default.static_ips.[0] ))

- name: upload-kibana-objects
  lifecycle: errand
  release: logsearch-for-cloudfoundry
  instances: 1
  templates:
  - {name: upload-kibana-objects, release: logsearch-for-cloudfoundry}
  networks:
  - name: default
  resource_pool: errand
  properties:
    elasticsearch:
      host: (( grab jobs.elasticsearch_master.networks.default.static_ips.[0] ))
      port: 9200

- name: create-uaa-client
  lifecycle: errand
  release: logsearch-for-cloudfoundry
  instances: 1
  templates:
  - {name: create-uaa-client, release: logsearch-for-cloudfoundry}
  networks:
  - name: default
  resource_pool: errand
  properties:
    create-uaa-client:
      oauth2_client_id: (( param "specify uaa client id" ))
      oauth2_client_secret: (( param "specify uaa client secret" ))
      cloudfoundry:
        system_domain: (( param "specify cf system domain" ))
        admin_client_secret: (( param "specify cf client secret" ))

# Global properties
properties:
  curator:
    elasticsearch:
      host: (( grab jobs.elasticsearch_master.networks.default.static_ips.[0] ))
      port: 9200
  logstash:
    plugins:
    - {name: logstash-output-cloudwatchlogs-latest, version: 2.0.0.pre1}
  logstash_parser:
    debug: false
  logstash_ingestor:
    debug: false
    outputs:
    - plugin: redis
      options: {}
    - plugin: s3
      options:
        region: (( grab meta.aws.region ))
        bucket: (( grab meta.ingestor.bucket ))
        time_file: 5
        server_side_encryption: true
    - plugin: cloudwatchlogs
      options:
        region: (( grab meta.aws.region ))
        log_group_name: (( concat "logsearch-" meta.env ))
        log_stream_name: "firehose-%instance_id%"
  redis:
    host: (( grab jobs.queue.networks.default.static_ips.[0] ))
  elasticsearch:
    master_hosts: (( grab jobs.elasticsearch_master.networks.default.static_ips ))
    cluster_name: logsearch
    exec:
      environment:
        JAVA_OPTS: '"-Djava.io.tmpdir=${TMP_DIR}"'
  kibana:
    elasticsearch:
      host: (( grab jobs.elasticsearch_master.networks.default.static_ips.[0] ))
      port: 9200
  elasticsearch_config:
    elasticsearch:
      host: (( grab jobs.elasticsearch_master.networks.default.static_ips.[0] ))
    templates:
    - index_template: /var/vcap/packages/logsearch-config/default-mappings.json
  snort:
    rules:
    - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"POST"; http_method; content: "logs-app"; http_uri; content:"/_update"; http_uri; classtype:web-application-attack; sid:343080002; rev:1;)'
    - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"POST"; http_method; content: "logs-platform"; http_uri; content:"/_update"; http_uri; classtype:web-application-attack; sid:343080003; rev:1;)'
    - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"DELETE"; http_method; content: "logs-app"; http_uri; classtype:web-application-attack; sid:343080004; rev:1;)'
    - (( concat "suppress gen_id 1, sig_id 343080004, track by_src, ip " jobs.cluster_monitor.networks.default.static_ips.[0] ))
    - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"DELETE"; http_method; content: "logs-platform"; http_uri; classtype:web-application-attack; sid:343080005; rev:1;)'
    - (( concat "suppress gen_id 1, sig_id 343080005, track by_src, ip " jobs.cluster_monitor.networks.default.static_ips.[0] ))
